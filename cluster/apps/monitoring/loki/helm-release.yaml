---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: loki
spec:
  interval: 5m
  chart:
    spec:
      # renovate: registryUrl=https://grafana.github.io/helm-charts
      chart: loki
      version: 2.12.0
      sourceRef:
        kind: HelmRepository
        name: grafana-charts
        namespace: flux-system
      interval: 5m
  values:
    serviceMonitor:
      enabled: true
    config:
      limits_config:
        reject_old_samples: false
      storage_config:
        aws:
          s3: s3://minio.tools:9000/loki"
          bucketnames: loki
          endpoint: minio.tools:9000
          access_key_id: "${SECRET_MINIO_ACCESS_KEY}"
          secret_access_key: "${SECRET_MINIO_SECRET_KEY}"
          s3forcepathstyle: true
          # http_config:
          #   insecure_skip_verify: true
          insecure: true
        boltdb_shipper:
          resync_interval: 5s
          shared_store: s3
        filesystem: null
      schema_config:
        configs:
          - from: "2020-10-24"
            store: boltdb-shipper
            object_store: aws
            schema: v11
            index:
              prefix: index_
              period: 24h
      ruler:
        storage:
          type: local
          local:
            directory: /rules
        rule_path: /tmp/scratch
        alertmanager_url: http://prometheus-alertmanager:9093
        ring:
          kvstore:
            store: inmemory
        enable_api: true
    persistence:
      enabled: false
    alerting_groups:
      #
      # SMART Failures
      #
      - name: smart-failure
        rules:
          - alert: SmartFailures
            expr: |
              sum by (hostname) (count_over_time({hostname=~".+"} |~ "(?i).*smartd.*(error|fail).*"[2m])) > 0
            for: 10s
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "SMART has reported failures a drive on {{ $hostname }}"
      # #
      # # *arr applications
      # #
      # - name: arr
      #   rules:
      #     - alert: ArrDatabaseIsLocked
      #       expr: |
      #         sum by (app) (count_over_time({app=~".*arr"} |~ "(?i)database is locked"[2m])) > 0
      #       for: 10s
      #       labels:
      #         severity: critical
      #         category: logs
      #       annotations:
      #         summary: "{{$app}} is experiencing locked database issues"
