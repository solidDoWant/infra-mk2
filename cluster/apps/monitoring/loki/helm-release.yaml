---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: loki
spec:
  interval: 5m
  chart:
    spec:
      # renovate: registryUrl=https://grafana.github.io/helm-charts
      chart: loki
      version: 2.6.0
      sourceRef:
        kind: HelmRepository
        name: grafana-charts
        namespace: flux-system
      interval: 5m
  values:
    # ingress:
    #   enabled: true
    #   annotations:
    #     kubernetes.io/ingress.class: "traefik"
    #     traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
    #   hosts:
    #     - host: "loki.${SECRET_DOMAIN}"
    #       paths:
    #         - /
    #   tls:
    #     - hosts:
    #         - "loki.${SECRET_DOMAIN}"
    serviceMonitor:
      enabled: true
    config:
      storage_config:
        aws:
          bucketnames: loki
          endpoint: minio.tools:9000
          access_key_id: "${SECRET_MINIO_ACCESS_KEY}"
          secret_access_key: "${SECRET_MINIO_SECRET_KEY}"
          s3forcepathstyle: true
          insecure: true
        boltdb_shipper:
          resync_interval: 5s
          shared_store: s3
      ruler:
        storage:
          type: local
          local:
            directory: /rules
        rule_path: /tmp/scratch
        alertmanager_url: http://prometheus-alertmanager:9093
        ring:
          kvstore:
            store: inmemory
        enable_api: true
    persistence:
      enabled: true
      storageClassName: zfs-iscsi-bulk-csi
    alerting_groups:
      #
      # SMART Failures
      #
      - name: smart-failure
        rules:
          - alert: SmartFailures
            expr: |
              sum by (hostname) (count_over_time({hostname=~".+"} |~ "(?i).*smartd.*(error|fail).*"[2m])) > 0
            for: 10s
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "SMART has reported failures a drive on {{ hostname }}"
      # #
      # # *arr applications
      # #
      # - name: arr
      #   rules:
      #     - alert: ArrDatabaseIsLocked
      #       expr: |
      #         sum by (app) (count_over_time({app=~".*arr"} |~ "(?i)database is locked"[2m])) > 0
      #       for: 10s
      #       labels:
      #         severity: critical
      #         category: logs
      #       annotations:
      #         summary: "{{$app}} is experiencing locked database issues"
